{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'praw'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-27d6c54c4a84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpraw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'praw'"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import sys\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os.path\n",
    "import prawcore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(created):\n",
    "    return dt.datetime.fromtimestamp(created)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your keys/secrets as strings in the following fields\n",
    "# credentials = {}\n",
    "# credentials['client_id'] = 'Tt3sc9zHX1U4Pg'\n",
    "# credentials['client_secret'] = 'Tl_rWZZtVo0k46FFkM2i0BBCWQM'\n",
    "# credentials['user_agent'] = 'Scraping_data'\n",
    "# credentials['username'] = '311Sheetal'\n",
    "# credentials['password'] = 'Reddit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the credentials object to file\n",
    "# with open(\"reddit_credentials.json\", \"w\") as file:\n",
    "#           json.dump(credentials, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials from json file\\n\n",
    "with open(\"reddit_credentials.json\", \"r\") as file:\n",
    "    creds = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id = creds['client_id'],\n",
    "                     client_secret = creds['client_secret'],\n",
    "                     user_agent = creds['user_agent'],\n",
    "                     username = creds['username'],\n",
    "                     password = creds['password'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Reddit Subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = {'republicans' : ['Republican'],\n",
    "              'democrats': ['democrats']\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Posts and Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_posts(reddit_instance, subreddits, limit_posts=100):\n",
    "    \n",
    "    subreddit_submissions_dict = {\"created\":[],\n",
    "                              \"title\":[],\n",
    "                              \"score\":[],\n",
    "                              \"post_id\": [],\n",
    "                              \"subreddit_id\": [],\n",
    "                              \"subreddit\" : [],\n",
    "                              \"author\" : [],\n",
    "                              \"title\":[],\n",
    "                              \"upvote_ratio\": [],\n",
    "                             \"body\": [],\n",
    "                             \"url\": [],\n",
    "                             \"num_comments\":[],\n",
    "                                 \"group\": []}\n",
    "\n",
    "    for i in subreddits:\n",
    "        for j in subreddits[i]:\n",
    "            subreddit = reddit.subreddit(j)\n",
    "            for submission in tqdm(subreddit.new(limit=limit_posts), total = limit_posts, file=sys.stdout):\n",
    "                if (not submission.banned_by is None) or (not submission.author is '[Deleted]') or (not submission.selftext == '[deleted]') or (not submission.selftext == '[removed]'):\n",
    "                    subreddit_submissions_dict['created'].append(submission.created)\n",
    "                    subreddit_submissions_dict['title'].append(submission.title)\n",
    "                    subreddit_submissions_dict['score'].append(submission.score)\n",
    "                    subreddit_submissions_dict['post_id'].append(submission.id)\n",
    "                    subreddit_submissions_dict['subreddit_id'].append(submission.subreddit_id)\n",
    "                    subreddit_submissions_dict['subreddit'].append(submission.subreddit)\n",
    "                    subreddit_submissions_dict['author'].append(submission.author)\n",
    "                    subreddit_submissions_dict['num_comments'].append(submission.num_comments)\n",
    "                    subreddit_submissions_dict['upvote_ratio'].append(submission.upvote_ratio)\n",
    "                    subreddit_submissions_dict['body'].append(submission.selftext)\n",
    "                    subreddit_submissions_dict['url'].append(submission.url)\n",
    "                    subreddit_submissions_dict['group'].append(i)\n",
    "                    \n",
    "    subreddit_data = pd.DataFrame(subreddit_submissions_dict)\n",
    "    _timestamp = subreddit_data[\"created\"].apply(get_date)\n",
    "    subreddit_data = subreddit_data.assign(timestamp = _timestamp)\n",
    "                    \n",
    "    return subreddit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_comments(reddit_instance,postids):\n",
    "    \n",
    "    comments_dict = {\n",
    "        \"created\": [],\n",
    "        \"comment_id\": [],\n",
    "        \"author\": [],\n",
    "        \"body\": [],\n",
    "        \"parent_id\":[],\n",
    "        \"submission_id\":[],\n",
    "        \"score\":[],\n",
    "        \"subreddit\":[],\n",
    "        \"subreddit_id\":[]\n",
    "    }\n",
    "\n",
    "    for postid in tqdm(postids, total = len(post_ids), file=sys.stdout):\n",
    "        submission = reddit_instance.submission(postid)\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        for comment in submission.comments.list():\n",
    "            if comment.body != \"[removed]\" and comment.author != None:\n",
    "                comments_dict['created'].append(comment.created_utc)\n",
    "                comments_dict['comment_id'].append(comment.id)\n",
    "                comments_dict['author'].append(comment.author)\n",
    "                comments_dict['body'].append(comment.body)\n",
    "                comments_dict['parent_id'].append(comment.parent_id)\n",
    "                comments_dict['submission_id'].append(postid)\n",
    "                comments_dict['score'].append(comment.score)\n",
    "                comments_dict['subreddit'].append(comment.subreddit)\n",
    "                comments_dict['subreddit_id'].append(comment.subreddit_id)\n",
    "            \n",
    "    return pd.DataFrame(comments_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts File Exists!\n",
      "Read File!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('posts_group.csv'):\n",
    "    print(\"Posts File Exists!\")\n",
    "    subreddit_data = pd.read_csv('posts_group.csv')\n",
    "    print(\"Read File!\")\n",
    "else:\n",
    "    # pull posts from the group of subreddits\n",
    "    print(\"Pulling Subreddits!\")\n",
    "    limit_posts = 1000\n",
    "    subreddit_data = pull_posts(reddit, subreddits=subreddits, limit_posts= limit_posts)\n",
    "    subreddit_data.to_csv('posts_group.csv', index = False)\n",
    "    print(\"Pulled Posts from Subreddits!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments File Exists!\n",
      "Read File!\n"
     ]
    }
   ],
   "source": [
    "# make dictionary of dataframes for each group\n",
    "if os.path.exists(\"comments_group.csv\"):\n",
    "    print(\"Comments File Exists!\")\n",
    "    subreddit_data = pd.read_csv('posts_group.csv')\n",
    "    print(\"Read File!\")\n",
    "\n",
    "else:\n",
    "    print(\"Pulling Comments from each Post!\")\n",
    "    groups_posts = {}\n",
    "    for i in subreddits:\n",
    "        post_ids = subreddit_data.loc[subreddit_data.group==i].post_id.values\n",
    "        groups_posts_df = fetch_comments(reddit, postids=post_ids)\n",
    "        groups_posts[i] = groups_posts_df  \n",
    "    # make a dataframe of users for each post thread\n",
    "    users_df = pd.concat(groups_posts, keys = groups_posts.keys()).reset_index().rename({'level_0':'group'},axis =\"columns\").drop(\"level_1\", axis = 1)\n",
    "    users_df.to_csv('comments_group.csv', index = False)\n",
    "    print(\"Pulled Comments from each Post!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make username dictionary for each group\n",
    "\n",
    "user_df = pd.read_csv(\"comments_group.csv\")\n",
    "usernames = {}\n",
    "for i in subreddits:\n",
    "    usernames[i] = list()\n",
    "    \n",
    "for i in subreddits:\n",
    "    usernames[i].extend(list(set(user_df.loc[user_df['group'] == i]['author'].values)))\n",
    "    usernames[i].extend(list(set(subreddit_data.loc[subreddit_data['group'] == i]['author'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get User Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_user():\n",
    "    \n",
    "    republican_users=[]\n",
    "    democrat_users=[]\n",
    "    \n",
    "    \n",
    "    # with user inclination\n",
    "\n",
    "    for i in usernames.keys():\n",
    "        count=0\n",
    "        for author in usernames[i]:\n",
    "            count_democrats=0\n",
    "            count_republican=0\n",
    "            count+=1\n",
    "            if count==100:\n",
    "                break\n",
    "            user = reddit.redditor(author)\n",
    "            user_groups = {#\"user_id\":[],\n",
    "                            \"user\":[],\n",
    "                          #\"comment\":[],\n",
    "                          \"subreddit\":[]}\n",
    "            try:\n",
    "                #get comments of that particular user\n",
    "                for c in user.comments.new(limit=None):\n",
    "                    #user_groups['user_id'].append(user.id)\n",
    "                    if c.subreddit==\"Republican\" or c.subreddit==\"democrats\":\n",
    "                        user_groups['user'].append(author)\n",
    "                        user_groups['subreddit'].append(c.subreddit)\n",
    "                    #user_groups['comment'].append(c.body)\n",
    "                    #user_groups['replies'].append(c.replies)\n",
    "            except prawcore.exceptions.NotFound:\n",
    "                continue\n",
    "            except prawcore.exceptions.Forbidden:\n",
    "                continue\n",
    "            user_comments = []\n",
    "            user_comments = pd.DataFrame(user_groups)\n",
    "\n",
    "            all_subreddits =[]\n",
    "            all_subreddits = user_comments['subreddit'].tolist()\n",
    "\n",
    "            count_republican = all_subreddits.count('Republican')\n",
    "            count_democrats = all_subreddits.count('democrats')\n",
    "\n",
    "            if count_republican>=count_democrats:\n",
    "                republican_users.append(author)\n",
    "\n",
    "            else:\n",
    "                democrat_users.append(author)\n",
    "                \n",
    "       \n",
    "    user_groups_dict={#\"user_id\":[],\n",
    "            \"user\":[],\n",
    "          #\"comment\":[],\n",
    "          \"subreddit\":[]}\n",
    "    for i in republican_users:\n",
    "        user_groups_dict['user'].append(i)\n",
    "        user_groups_dict['subreddit'].append('Republican')\n",
    "\n",
    "    for i in democrat_users:\n",
    "        user_groups_dict['user'].append(i)\n",
    "        user_groups_dict['subreddit'].append('democrats')\n",
    "    user_groups = pd.DataFrame(user_groups_dict)    \n",
    "    user_groups.to_csv('user_groups.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Group File Exists!\n",
      "Read File!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"user_groups.csv\"):\n",
    "    print(\"User Group File Exists!\")\n",
    "    user_groups = pd.read_csv('user_groups.csv')\n",
    "    print(\"Read File!\") \n",
    "else:\n",
    "    group_user()\n",
    "    print(\"Done!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of subscribers to a particular subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_name = reddit.subreddit('Republican')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_name.subscribers  #the names of the subscribers is private"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
