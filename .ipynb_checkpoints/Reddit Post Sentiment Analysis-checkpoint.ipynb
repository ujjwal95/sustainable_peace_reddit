{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from reddit_helpers.text_processor import reddit_text_preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Cred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials from json file\\n\n",
    "with open(\"reddit_credentials.json\", \"r\") as file:\n",
    "    creds = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'client_id': 'Tt3sc9zHX1U4Pg',\n",
       " 'client_secret': 'Tl_rWZZtVo0k46FFkM2i0BBCWQM',\n",
       " 'user_agent': 'Scraping_data',\n",
       " 'username': '311Sheetal',\n",
       " 'password': 'Reddit'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id = creds['client_id'],\n",
    "                     client_secret = creds['client_secret'],\n",
    "                     user_agent = creds['user_agent'],\n",
    "                     username = creds['username'],\n",
    "                     password = creds['password'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(created):\n",
    "    return dt.datetime.fromtimestamp(created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_comments(reddit_instance,postids):\n",
    "    \n",
    "    comments_dict = {\n",
    "        \"created\": [],\n",
    "        \"comment_id\": [],\n",
    "        \"author\": [],\n",
    "        \"body\": [],\n",
    "        \"parent_id\":[],\n",
    "        \"submission_id\":[],\n",
    "        \"score\":[],\n",
    "        \"subreddit\":[],\n",
    "        \"subreddit_id\":[]\n",
    "    }\n",
    "\n",
    "    submission = reddit_instance.submission(postids)\n",
    "    for postid in postids:\n",
    "        submission = reddit_instance.submission(postid)\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        for comment in submission.comments.list():\n",
    "            comments_dict['created'].append(comment.created_utc)\n",
    "            comments_dict['comment_id'].append(comment.id)\n",
    "            comments_dict['author'].append(comment.author)\n",
    "            comments_dict['body'].append(comment.body)\n",
    "            comments_dict['parent_id'].append(comment.parent_id)\n",
    "            comments_dict['submission_id'].append(postid)\n",
    "            comments_dict['score'].append(comment.score)\n",
    "            comments_dict['subreddit'].append(comment.subreddit)\n",
    "            comments_dict['subreddit_id'].append(comment.subreddit_id)\n",
    "            \n",
    "    return pd.DataFrame(comments_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Comments of a Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_info = fetch_comments(reddit, ['9u948a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_timestamp = comments_info[\"created\"].apply(get_date)\n",
    "comments_info = comments_info.assign(timestamp = _timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check linked users\n",
    "comments_info['linked_users'] = comments_info['body'].apply(lambda x: re.findall('/u/[A-Za-z0-9_-]+',x))\n",
    "# check linked subreddits\n",
    "comments_info['linked_subreddits'] = comments_info['body'].apply(lambda x: re.findall('r/[A-Za-z0-9_-]+',x))\n",
    "\n",
    "# remove numbers etc\n",
    "comments_info['processed_body'] = comments_info['body'].str.replace(\"[^a-zA-Z#]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process text\n",
    "comments_info['processed_body'] = comments_info['processed_body'].apply(lambda x: reddit_text_preprocessing(x).replace_abbreviations().remove_short_words().lower_case().process_html().remove_urls().decode_text().stopwords_remove().stopwords_remove().lemmatize().text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>submission_id</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>linked_users</th>\n",
       "      <th>linked_subreddits</th>\n",
       "      <th>processed_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.541387e+09</td>\n",
       "      <td>e92pd71</td>\n",
       "      <td>TheLizardKing25</td>\n",
       "      <td>I’m republican and glad she lost but this is a...</td>\n",
       "      <td>t3_9u948a</td>\n",
       "      <td>9u948a</td>\n",
       "      <td>202</td>\n",
       "      <td>Republican</td>\n",
       "      <td>t5_2qndt</td>\n",
       "      <td>2018-11-04 22:06:03</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>republican glad lose terrible comparisignifica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.541424e+09</td>\n",
       "      <td>e93ctfa</td>\n",
       "      <td>Russilito</td>\n",
       "      <td>This is a bit of a stretch...</td>\n",
       "      <td>t3_9u948a</td>\n",
       "      <td>9u948a</td>\n",
       "      <td>15</td>\n",
       "      <td>Republican</td>\n",
       "      <td>t5_2qndt</td>\n",
       "      <td>2018-11-05 08:22:01</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>stretake careh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.541393e+09</td>\n",
       "      <td>e92uzr2</td>\n",
       "      <td>TakenStankForever</td>\n",
       "      <td>Are you trying to tell me that UCF *aren't* na...</td>\n",
       "      <td>t3_9u948a</td>\n",
       "      <td>9u948a</td>\n",
       "      <td>4</td>\n",
       "      <td>Republican</td>\n",
       "      <td>t5_2qndt</td>\n",
       "      <td>2018-11-04 23:35:41</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>try tell national champ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.541456e+09</td>\n",
       "      <td>e94fu5d</td>\n",
       "      <td>bisemutum</td>\n",
       "      <td>$100,000,000,000 Billion is 5 million times th...</td>\n",
       "      <td>t3_9u948a</td>\n",
       "      <td>9u948a</td>\n",
       "      <td>3</td>\n",
       "      <td>Republican</td>\n",
       "      <td>t5_2qndt</td>\n",
       "      <td>2018-11-05 17:20:07</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>billion million tinstant message illegal insta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.541391e+09</td>\n",
       "      <td>e92tnh4</td>\n",
       "      <td>Poopsmith89</td>\n",
       "      <td>I dont completely understand the scoring syste...</td>\n",
       "      <td>t3_9u948a</td>\n",
       "      <td>9u948a</td>\n",
       "      <td>7</td>\n",
       "      <td>Republican</td>\n",
       "      <td>t5_2qndt</td>\n",
       "      <td>2018-11-04 23:12:27</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>dont completely understand score soonte-mail e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        created comment_id             author  \\\n",
       "0  1.541387e+09    e92pd71    TheLizardKing25   \n",
       "1  1.541424e+09    e93ctfa          Russilito   \n",
       "2  1.541393e+09    e92uzr2  TakenStankForever   \n",
       "3  1.541456e+09    e94fu5d          bisemutum   \n",
       "4  1.541391e+09    e92tnh4        Poopsmith89   \n",
       "\n",
       "                                                body  parent_id submission_id  \\\n",
       "0  I’m republican and glad she lost but this is a...  t3_9u948a        9u948a   \n",
       "1                      This is a bit of a stretch...  t3_9u948a        9u948a   \n",
       "2  Are you trying to tell me that UCF *aren't* na...  t3_9u948a        9u948a   \n",
       "3  $100,000,000,000 Billion is 5 million times th...  t3_9u948a        9u948a   \n",
       "4  I dont completely understand the scoring syste...  t3_9u948a        9u948a   \n",
       "\n",
       "   score   subreddit subreddit_id           timestamp linked_users  \\\n",
       "0    202  Republican     t5_2qndt 2018-11-04 22:06:03           []   \n",
       "1     15  Republican     t5_2qndt 2018-11-05 08:22:01           []   \n",
       "2      4  Republican     t5_2qndt 2018-11-04 23:35:41           []   \n",
       "3      3  Republican     t5_2qndt 2018-11-05 17:20:07           []   \n",
       "4      7  Republican     t5_2qndt 2018-11-04 23:12:27           []   \n",
       "\n",
       "  linked_subreddits                                     processed_body  \n",
       "0                []  republican glad lose terrible comparisignifica...  \n",
       "1                []                                     stretake careh  \n",
       "2                []                            try tell national champ  \n",
       "3                []  billion million tinstant message illegal insta...  \n",
       "4                []  dont completely understand score soonte-mail e...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use google language api\n",
    "\n",
    "# def gc_sentiment(text, credentials):  \n",
    "#     from google.cloud import language\n",
    "    \n",
    "#     client = language.LanguageServiceClient(credentials = credentials)\n",
    "#     document = language.types.Document(\n",
    "#             content=text,\n",
    "#             type=language.enums.Document.Type.PLAIN_TEXT)\n",
    "#     annotations = client.analyze_sentiment(document=document)\n",
    "#     score = annotations.document_sentiment.score\n",
    "#     magnitude = annotations.document_sentiment.magnitude\n",
    "#     return score, magnitude\n",
    "\n",
    "# import os\n",
    "# from google.oauth2 import service_account\n",
    "# credentials = service_account.Credentials.from_service_account_file('./ecbm4040-up2138-b37eacd8e36c.json')\n",
    "# print('Credendtials from environ: {}'.format(os.environ.get('GOOGLE_APPLICATION_CREDENTIALS')))\n",
    "\n",
    "# gc_results = [gc_sentiment(row, credentials) for row in tqdm(comments_info['processed_body'])]\n",
    "# gc_score, gc_magnitude = zip(*gc_results) # Unpacking the result into 2 lists\n",
    "# gc = list(zip(comments_info['processed_body'], gc_score, gc_magnitude))\n",
    "# columns = ['text', 'score', 'magnitude']\n",
    "# gc_df = pd.DataFrame(gc, columns = columns)\n",
    "\n",
    "# gc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SIA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_score = sia.polarity_scores(comments_info.body[102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.178, 'neu': 0.791, 'pos': 0.03, 'compound': -0.8997}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You realize I never said freeloaders shouldn't be able to vote, right.  And you realize I corrected the previous poster on that point already, right.\\n\\nAnd you realize I'm focusing on addressing issues with my comments, right.  So the other points you reference are not germaine.  For that matter I could also start talking about the illegal immigrants issue and the annual $100,000,000,000 Billion price tag on that. And the chief culprit again being California.\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_info.body[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All your counterpoints just to rationalize your wild left zealotry and ideologies.  You can square it anyway you desire, but it will never be squared in our wallets and pocketbooks where it counts - in reality!  I guess we have to actually bankrupt ourselves before people wake up to these very cruel facts.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_info.body[102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
